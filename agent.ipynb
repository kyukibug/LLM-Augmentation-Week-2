{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll build a basic LangChain agent. We'll leverage OpenAI for LLM responses and implement various tools to create a flexible, dynamic workflow. LangChain's agent framework will allow us to orchestrate different functions and API calls seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "import os\n",
    "from rag import RagEngine \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Decorator Approach\n",
    "\n",
    "The `@tool` decorator in LangChain is used to easily register functions as tools that agents can use. It simplifies integration by marking the function for use in LangChain workflows without needing extra configuration.\n",
    "\n",
    "In this case, the `@tool` decorator:\n",
    "- Registers the `calculator` function as a callable tool for agents.\n",
    "- Allows the agent to evaluate mathematical expressions safely by using `eval()` with restricted input handling.\n",
    "\n",
    "This approach makes it easy to add modular, reusable tools to LangChain agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculator(input_expr: str) -> int:\n",
    "    \"\"\"A simple calculator tool that evaluates a mathemtical expression and returns the result as an integer. Do not pass untrusted input.\"\"\"\n",
    "    return eval(input_expr.replace(\"^\", \"**\"), {}, {})\n",
    "\n",
    "tools = [calculator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3v/5vb40xkj31j1vcg2rjkwvm780000gn/T/ipykernel_83678/823011281.py:3: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
      "  zero_shot_agent = initialize_agent(\n",
      "/var/folders/3v/5vb40xkj31j1vcg2rjkwvm780000gn/T/ipykernel_83678/823011281.py:11: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  zero_shot_agent(\"what is (4.5*2.1)^2.2?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the calculator tool to evaluate this mathematical expression.\n",
      "Action: calculator\n",
      "Action Input: (4.5*2.1)^2.2\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m139.94261298333066\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 139.94\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is (4.5*2.1)^2.2?', 'output': '139.94'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "zero_shot_agent = initialize_agent(\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "zero_shot_agent(\"what is (4.5*2.1)^2.2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tools for the Agent\n",
    "\n",
    "In this section, we define two tools that the agent will use:\n",
    "\n",
    "1. **QA Chat Tool**: A simple question-answering tool that takes a user message and generates a response using a large language model (LLM).\n",
    "2. **RAG Chat Tool**: A more complex tool that handles document retrieval using Retrieval-Augmented Generation (RAG). This tool is specifically designed to retrieve and generate responses from environmental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the QA Chat Tool\n",
    "# This tool handles normal question-answering interactions by invoking the LLM\n",
    "@tool\n",
    "def qa_chat_tool(message: str) -> str:\n",
    "    \"\"\"Tool for handling normal question-answer (QA) interactions.\"\"\"\n",
    "\n",
    "    template = \"\"\"Respond to the user message:\n",
    "        message: {message}\n",
    "        \"\"\"\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke(message)\n",
    "    return response # Return the generated response\n",
    "\n",
    "# Define the RAG Chat Tool: Feel free to change the rag document and description to add a different document\n",
    "# This tool handles document retrieval and generation using RAG from environmental data\n",
    "@tool\n",
    "def rag_chat_tool(message: str) -> str:\n",
    "    \"\"\"Tool for handling document retrieval and generating responses using RAG from environmental data.\"\"\"\n",
    "    \n",
    "    # Initialize the Retrieval-Augmented Generation (RAG) engine\n",
    "    rag_engine = RagEngine()\n",
    "    \n",
    "    # Use the RAG engine to return an answer to the user's query\n",
    "    response = rag_engine.return_answer(message)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Calling Application with Tools in LangChain\n",
    "\n",
    "In this example, we are creating a function-calling agent using LangChain. The agent is set up with tools such as `qa_chat_tool` and `rag_chat_tool`, which can be used for question-answering and retrieval-augmented generation tasks. The agent is initialized as a zero-shot agent, meaning it can take actions based on descriptions of the tools provided to it without requiring training on specific tasks.\n",
    "\n",
    "The `max_iterations` parameter limits the number of decision-making steps the agent can take before completing a task. Here's how to set up and initialize the agent with tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools the agent will use\n",
    "tools = [qa_chat_tool, rag_chat_tool]\n",
    "\n",
    "# Initialize the zero-shot agent\n",
    "# The agent is based on the \"zero-shot-react-description\" model, meaning it uses \n",
    "# reasoning and action descriptions to perform tasks without explicit training\n",
    "zero_shot_agent = initialize_agent(\n",
    "    agent=\"zero-shot-react-description\",  # Agent type for reasoning and decision making\n",
    "    tools=tools,  # List of tools the agent will have access to\n",
    "    llm=llm,  # The large language model (LLM) that powers the agent\n",
    "    verbose=True,  # Set to True to see detailed output of the agent's decision-making process\n",
    "    max_iterations=3  # Limit the agent to 3 iterations when performing actions\n",
    ")\n",
    "\n",
    "# You can now use zero_shot_agent to run function calls based on input queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the RAG chat tool to retrieve information on the cause of global warming.\n",
      "Action: rag_chat_tool\n",
      "Action Input: \"What is the cause of global warming?\"\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bug/mdst/LLM-Augmentation-Week-2/rag.py:29: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  self.chunks, embedding=OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe cause of global warming is believed to be an enhanced greenhouse effect, with a marked increase in energy consumption leading to a rise in atmospheric carbon dioxide concentration.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should provide this information to answer the question.\n",
      "Action: qa_chat_tool\n",
      "Action Input: \"The cause of global warming is believed to be an enhanced greenhouse effect, with a marked increase in energy consumption leading to a rise in atmospheric carbon dioxide concentration.\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mYes, that is correct. The enhanced greenhouse effect is primarily caused by human activities such as burning fossil fuels, deforestation, and industrial processes, which release large amounts of greenhouse gases like carbon dioxide into the atmosphere. This increase in greenhouse gases traps heat and leads to a rise in global temperatures, resulting in global warming. It is important for us to reduce our energy consumption and transition to cleaner, renewable sources of energy to mitigate the impacts of climate change.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The cause of global warming is primarily attributed to an enhanced greenhouse effect, driven by human activities such as burning fossil fuels, deforestation, and industrial processes that release greenhouse gases into the atmosphere.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the cause of global warming?',\n",
       " 'output': 'The cause of global warming is primarily attributed to an enhanced greenhouse effect, driven by human activities such as burning fossil fuels, deforestation, and industrial processes that release greenhouse gases into the atmosphere.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This one may take a while oops\n",
    "zero_shot_agent(\"what is the cause of global warming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the QA tool to get a straightforward answer to this question.\n",
      "Action: qa_chat_tool\n",
      "Action Input: \"What type of food is an apple?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAn apple is a type of fruit.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have the answer to the question.\n",
      "Final Answer: An apple is a type of fruit.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What type of food is an apple?',\n",
       " 'output': 'An apple is a type of fruit.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_agent(\"What type of food is an apple?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Your Turn\n",
    "\n",
    "Now, create a function-calling application of your own. You are encouraged to use external APIs to create robust tools. Feel free to change the rag engine to a more personalized document. You will may need to change rag.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the whoIsCool tool to determine if Alex is cool or not.\n",
      "Action: whoIsCool\n",
      "Action Input: \"Alex\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mis cool\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have determined that Alex is cool.\n",
      "Final Answer: Alex is cool.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Is alex cool?', 'output': 'Alex is cool.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# says who is cool, takes in a name\n",
    "@tool\n",
    "def whoIsCool(message: str) -> str:\n",
    "    \"\"\"Tool for handling who is cool, takes in a name outputs if cool or not.\"\"\"\n",
    "    if message == 'Alex':\n",
    "        return \"is cool\"\n",
    "    return 'not cool'\n",
    "\n",
    "\n",
    "tools = [whoIsCool]\n",
    "\n",
    "zero_shot_agent = initialize_agent(\n",
    "    agent=\"zero-shot-react-description\",  # Agent type for reasoning and decision making\n",
    "    tools=tools,  # List of tools the agent will have access to\n",
    "    llm=llm,  # The large language model (LLM) that powers the agent\n",
    "    verbose=True,  # Set to True to see detailed output of the agent's decision-making process\n",
    "    max_iterations=3  # Limit the agent to 3 iterations when performing actions\n",
    ")\n",
    "\n",
    "zero_shot_agent(\"Is alex cool?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
